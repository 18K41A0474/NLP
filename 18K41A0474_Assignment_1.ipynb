{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18K41A0474_Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMT0Q6SeqJza8nJ+Olf/2D0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18K41A0474/NLP/blob/main/18K41A0474_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOBa1PBUjsKZ",
        "outputId": "04048f4c-5bf5-41bb-b586-43095db43106"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHw6ADy0j09N",
        "outputId": "247c2e18-09c4-448f-f403-16b1ab26931b"
      },
      "source": [
        "#1)Sentence tokenizing\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = '''Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?  Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.'''\n",
        "text1=''\n",
        "text1=sent_tokenize(text)\n",
        "for i in text1:\n",
        "    print(i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?\n",
            "Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin?\n",
            "Machines, after all, recognize numbers, not the letters of our language.\n",
            "And that can be a tricky landscape to navigate in machine learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heVgXNKFj6JQ",
        "outputId": "74bb0715-1ccd-4f86-b210-77abbcb37ad7"
      },
      "source": [
        "#2)word splitting\n",
        "from nltk.tokenize import word_tokenize\n",
        "text =  '''Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?  Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.'''\n",
        "text1=''\n",
        "text1=word_tokenize(text)\n",
        "for i in text1:\n",
        "    print(i)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are\n",
            "you\n",
            "fascinated\n",
            "by\n",
            "the\n",
            "amount\n",
            "of\n",
            "text\n",
            "data\n",
            "available\n",
            "on\n",
            "the\n",
            "internet\n",
            "?\n",
            "Are\n",
            "you\n",
            "looking\n",
            "for\n",
            "ways\n",
            "to\n",
            "work\n",
            "with\n",
            "this\n",
            "text\n",
            "data\n",
            "but\n",
            "aren\n",
            "’\n",
            "t\n",
            "sure\n",
            "where\n",
            "to\n",
            "begin\n",
            "?\n",
            "Machines\n",
            ",\n",
            "after\n",
            "all\n",
            ",\n",
            "recognize\n",
            "numbers\n",
            ",\n",
            "not\n",
            "the\n",
            "letters\n",
            "of\n",
            "our\n",
            "language\n",
            ".\n",
            "And\n",
            "that\n",
            "can\n",
            "be\n",
            "a\n",
            "tricky\n",
            "landscape\n",
            "to\n",
            "navigate\n",
            "in\n",
            "machine\n",
            "learning\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUmxssnAJxVi",
        "outputId": "d37522b7-e976-4c5b-919c-f06905bea106"
      },
      "source": [
        "#3)stem words\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "w=[\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "x=PorterStemmer()\n",
        "for i in w:\n",
        "    print(i,x.stem(i))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats cat\n",
            "trouble troubl\n",
            "troubling troubl\n",
            "troubled troubl\n",
            "having have\n",
            "Corriendo corriendo\n",
            "at at\n",
            "was wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNf1BvKdl2R3",
        "outputId": "8e594f58-b8df-4888-ff7d-efa8d2600a4b"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rme-4Zt4J4KK",
        "outputId": "fdbd085e-e707-47b3-b0c8-aacb04afd66a"
      },
      "source": [
        "#3)lemma words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "w=[\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "x= WordNetLemmatizer()\n",
        "for i in w:\n",
        "    print(i,x.lemmatize(i))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats cat\n",
            "trouble trouble\n",
            "troubling troubling\n",
            "troubled troubled\n",
            "having having\n",
            "Corriendo Corriendo\n",
            "at at\n",
            "was wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm8xcaxplAEc",
        "outputId": "fac5a113-2617-4ecb-d767-0bcca2c0d91d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBUDqautlEoi",
        "outputId": "31f75b4c-401a-4343-afb6-c84c285def2a"
      },
      "source": [
        "#4)stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "text=\"\"\"\n",
        "“The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.”\n",
        "\"\"\"\n",
        "words=word_tokenize(text)\n",
        "l=[]\n",
        "for w in words:\n",
        "    if w not in stop_words:\n",
        "        l.append(w)\n",
        "print(l)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['“', 'The', 'NLTK', 'library', 'one', 'oldest', 'commonly', 'used', 'Python', 'libraries', 'Natural', 'Language', 'Processing', '.', 'NLTK', 'supports', 'stop', 'word', 'removal', ',', 'find', 'list', 'stop', 'words', 'corpus', 'module', '.', 'To', 'remove', 'stop', 'words', 'sentence', ',', 'divide', 'text', 'words', 'remove', 'word', 'exits', 'list', 'stop', 'words', 'provided', 'NLTK', '.', '”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBuFFC_8lJR6",
        "outputId": "99cadc66-dc51-4a3c-f736-a81713aebe65"
      },
      "source": [
        "#5)printing frequency of each word\n",
        "import nltk\n",
        "from nltk.corpus import webtext\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "text=\"\"\"\n",
        "“The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.”\n",
        "\"\"\"\n",
        " \n",
        "w= word_tokenize(text)\n",
        "d = nltk.FreqDist(w)\n",
        "fw = dict([(m, n) for m, n in d.items() if len(m) > 3])\n",
        "for i in sorted(fw):\n",
        "    print(\"%s: %s\" % (i, fw[i]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language: 1\n",
            "NLTK: 3\n",
            "Natural: 1\n",
            "Processing: 1\n",
            "Python: 1\n",
            "commonly: 1\n",
            "corpus: 1\n",
            "divide: 1\n",
            "exits: 1\n",
            "find: 1\n",
            "from: 1\n",
            "into: 1\n",
            "libraries: 1\n",
            "library: 1\n",
            "list: 2\n",
            "module: 1\n",
            "most: 1\n",
            "oldest: 1\n",
            "provided: 1\n",
            "removal: 1\n",
            "remove: 2\n",
            "sentence: 1\n",
            "stop: 4\n",
            "supports: 1\n",
            "text: 1\n",
            "then: 1\n",
            "used: 1\n",
            "word: 2\n",
            "words: 4\n",
            "your: 1\n"
          ]
        }
      ]
    }
  ]
}